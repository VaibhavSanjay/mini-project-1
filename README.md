# Environment Prediction Algorithms for Robotic Reinforcement Learning
## Authors: Vaibhav Sanjay, Harshit Soora

Reinforcement learning has been a hot topic of discussion in robotics journals for algorithms that relate to robotic learning and decision making. Reinforcement learning algorithms have been shown to allow robots to learn tasks from navigation to household chores. However, RL often requires modification, adaptation, and enhancement by researchers into different types of solutions which best suit a particular problem definition. Thus, despite its success, RL solutions often don’t scale well and require extensive hyperparameter tuning for specific environments.

As of today, any new single agent RL research paper uses any of the following broader ideas to solve a problem
- **Provide more data**
  - Combine offline and online data via replay buffer
  - Gather more data during online training with World model or LLM

- **Do more training**
  - Combine offline and online learned policies
  - Combine on-policy and off-policy learned separately
  - Bootstrapping the agent with Imitation Learning

<We will add more research areas where RL is growing, with the help of our peers>

## Introduction
Perhaps the reason that RL algorithms fail to scale well to different environments may be related to a certain flaw that it shares with imitation learning methods, such as behavioral cloning: they learn nothing more than the action to take at their current time step. Specifically, they do not learn anything about their environment or what their future states will be, just that certain actions will give them a better reward in the long run. 

Recently however, researchers have been working on “imaginative” models that can somehow predict their environment, given that the agent performs a certain sequence of actions. It has been shown that these imaginative agents are superior to non-imaginative agents in terms of their robustness across various applications and lesser reliance on expert data. It is even possible for these agents to use the data they generate from imagining the future to train itself in an offline fashion, saving resources and posing less safety risks.

We discuss several recent methodologies and experiments involving the use of deep neural networks to understand and predict the world for reinforcement learning agents, and detail their potential uses in robotic decision making.

## Definition
This will be a literature review of Environment Prediction algorithms. We have selected a list of 3-4 papers, where we will talk about –
1) What is the key idea of the paper? 
2) How are the researchers trying to solve it?
 
It is already clear from the abstract why these ideas work in general, as they fall under the broader picture of providing more data to the Reinforcement Learning agent to train upon. How this is achieved is little different for each paper, DreamerV3 trains Actor-Critic on data generated by its World model (Neural Network), GenAug provides in more data from LLMs and does cropping and color jitter on top of it, PlanGAN trains conditional generative models (GANs) and train the agent on trajectories from the GAN, Future Image Similarity where agent is learning with future predicted images the help of CNNs. We will discuss it in more detail below.

## Key Papers
**PlanGAN**: https://arxiv.org/abs/2006.00900
- Application: Navigation, Reaching, Pick and Place

**Future Image Similarity**: https://arxiv.org/abs/1910.03157
- Application: Navigation

**Dreamerv3**: https://arxiv.org/abs/2301.04104
- Application: Video Games, Simulation

## Variant
**GenAug**: https://arxiv.org/abs/2302.06671
- Application: Data Augmentation

## Open Research Questions

## References

