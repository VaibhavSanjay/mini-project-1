# Environment Prediction Algorithms for Robotic Reinforcement Learning
## Authors: Vaibhav Sanjay, Harshit Soora

Reinforcement learning has been a hot topic of discussion in robotics journals for algorithms that relate to robotic learning and decision making. Reinforcement learning algorithms have been shown to allow robots to learn tasks from navigation to household chores. However, RL often requires modification, adaptation, and enhancement by researchers into different types of solutions which best suit a particular problem definition. Thus, despite its success, RL solutions often don’t scale well and require extensive hyperparameter tuning for specific environments.

As of today, any new single agent RL research paper uses any of the following broader ideas to solve a problem
- **Provide more data**
  - Combine offline and online data via replay buffer
  - Gather more data during online training with World model or LLM

- **Do more training**
  - Combine offline and online learned policies
  - Combine on-policy and off-policy learned separately
  - Bootstrapping the agent with Imitation Learning

## Introduction
Perhaps the reason that RL algorithms fail to scale well to different environments may be related to a certain flaw that it shares with imitation learning methods, such as behavioral cloning: they learn nothing more than the action to take at their current time step. Specifically, they do not learn anything about their environment or what their future states will be, just that certain actions will give them a better reward in the long run. 

Recently however, researchers have been working on “imaginative” models that can somehow predict their environment, given that the agent performs a certain sequence of actions. It has been shown that these imaginative agents are superior to non-imaginative agents in terms of their robustness across various applications and lesser reliance on expert data. It is even possible for these agents to use the data they generate from imagining the future to train itself in an offline fashion, saving resources and posing less safety risks.

We discuss several recent methodologies and experiments involving the use of deep neural networks to understand and predict the world for reinforcement learning agents, and detail their potential uses in robotic decision making.

## Definition
This will be a literature review of Environment Prediction algorithms. We have selected a list of 3-4 papers, where we will talk about –
1) What is the key idea of the paper? 
2) How are the researchers trying to solve it?
 
It is already clear from the abstract why these ideas work in general, as they fall under the broader picture of providing more data to the Reinforcement Learning agent to train upon. How this is achieved is little different for each paper, DreamerV3 trains Actor-Critic on data generated by its World model (Neural Network), GenAug provides in more data from LLMs and does cropping and color jitter on top of it, PlanGAN trains conditional generative models (GANs) and train the agent on trajectories from the GAN, Future Image Similarity where agent is learning with future predicted images the help of CNNs.

## Key Papers
### PlanGAN: https://arxiv.org/abs/2006.00900
Application: Navigation, Reaching, Pick and Place

#### Idea
<img width="607" alt="Screenshot 2024-10-07 at 4 11 22 PM" src="https://github.com/user-attachments/assets/7ce3d42f-dbe6-4743-b645-d98d058a338c">

This paper develops a model-based algorithm PlanGAN. The paper identifies the problem of performing reinforcement learning in environments with sparse rewards: such as those where rewards are only assigned when tasks are completed. Due to a lack of feedback during the rollout of the agent, training can be especially tricky. To address this issue, the authors develop a general algorithm that can train an ensemble of conditional Generative Adversarial Networks (GANs) that can produce feasible trajectories to the goals. The imagined trajectories from GANs are used to train the agent.

#### Key Contribution
<img width="609" alt="Screenshot 2024-10-07 at 4 12 14 PM" src="https://github.com/user-attachments/assets/b6228d39-cded-43a1-a7be-28b46edd6f57">

The authors employed a model based approach. As from their problem setting, it is a sparse reward environment where the reward is {0, 1}: 1 if it achieves the goal and 0 otherwise.
Imagined trajectories from GANs do not necessarily need to be optimal in the sense of moving directly towards the goal, since the second key component of their method involves feeding these proposed trajectories into a planning algorithm that decides which action to take in order to achieve the goal in as few steps as possible. They specifically use WGANs as these are more stable if training data is non-stationary.
Each of the N conditional-GANs has a generator and discriminator. Sample goals are taken randomly from a replay buffer, which is initially empty but is later filled with trajectories from the GANs. The GANs generate end to end trajectories during training but not during deployment.

##### What was done in case of Sparse reward so far
People used model-free RL but it required many trajectories to actually learn. Currently people are also trying model-based RL but have bias towards non-optimal actions and poor asymptotic performance.

#### Why it worked
Not only they provided extra data for the agent to train upon but also they provided specific data that teaches the agent to learn how to reach a goal. So in any spare reward setting these specialized trajectories generated by GANs are going to allow agents to converge faster. 
In our opinion, 5 GAN is supposed to work better than 3 GAN due to better generalization of GANs.

#### Open Challenges
The paper identified a noise generator, but it is rather unclear for the reasoning behind this within their structure. Future work could discuss methods besides Gaussian noise and generalize to different types of noises. Additionally, it will also not scale effectively if a new goal is added into the same environment due to the reason that the GANs are trained towards existing goals.

### Future Image Similarity: https://arxiv.org/abs/1910.03157
Application: Navigation

#### Idea
The authors use expert trajectories to learn a future scene predictor that can be used to train a robot through reinforcement learning, thus not requiring to deploy agent into the environment (ie. no self exploration). This idea is built on top of recent use of CNNs in RL. Behavioral cloning exists as prior work in this field but requires extensive expert data which this paper is trying to fill in. Finally they created an action-conditioned image predictor using CNN.

#### Key Contribution
<img width="433" alt="Screenshot 2024-10-07 at 4 17 06 PM" src="https://github.com/user-attachments/assets/31b44f9e-88f8-461d-b7b0-fb04568dd4cb">

This problem is similar to zero-trail (only expert demonstrations) imitation learning. The CNN model will generate future image (state) and actions. The CNN is trained using image-action pairs per timestamp from the expert data. Their CNN had two neural networks - Enc & Dec to learn future images, while another neural network was trained to learn actions - Act. 
Used a critique to identify the difference between expert trajectory next image and next image generated by CNNs. This became their loss function.

#### Why it worked
This is again a way to provide more training data but not using experts or exploration but creating your own model that will generate state - action data. The reason this method worked better than behavioral cloning was due to the power of the CNN to extract out features from expect trajectory image at time t, and using the noise of CNNs (that was generated due to only using expert trajectory) to create more imagery for exploration.

#### Open Challenges
From the way of how many neural networks are thrown at this problem, it looks like it will be very difficult to converge, and might require plenty of expert data. It was also not extensively tested for multiple different environments like DreamerV3, so it is difficult to say whether the findings in this paper extend to various robotic decision making scenarios. Increasing the breadth of tasks for this model could be useful in future work.

### Dreamerv3: https://arxiv.org/abs/2301.04104
Application: Video Games, Simulation

#### Idea
This paper developed a novel framework Dreamerv3. The paper identifies the problem that reinforcement learning algorithms tend to be very domain or problem specific: Once you change any number of parameters in your task, you might require a large number of changes in your network, from hyperparameters to architecture or computational resources. The hope from this paper is that a general algorithm that can learn” a world model that equips the agent with rich perception and the ability to imagine the future” will be better tasked to handle a wider range of practical applications. 
While typical reinforcement learning algorithms often follow an “actor-critic” approach, Dreamer introduces a third neural network to this approach: The world model network. All three networks are trained simultaneously. 

#### Key Contribution
<img width="613" alt="Screenshot 2024-10-07 at 4 13 17 PM" src="https://github.com/user-attachments/assets/7c3daf70-e84b-4511-8061-9cb6966df8dc">

The actor and critic neural networks remain mostly unchanged from the way they are in previous literature, so we focus our discussion on the world model network. The world model network is implemented as a “Recurrent State-Space Model (RSSM).” Using an encoder-decoder setup, the world model is able to encode the current view it sees from its camera and is able to use the encoded information and recurrent state to both train the decoder and predict the rewards. Thus, the model is trained to take prior states and actions to perceive the environment and predict future views.

<img width="638" alt="Screenshot 2024-10-07 at 4 13 30 PM" src="https://github.com/user-attachments/assets/7d187284-1bfd-48c0-a0af-53e84946794d">

Notably, to train the network, the authors employed a loss function that utilizes several pieces of information to train the model in various ways. The prediction loss is simply the loss of the decoder to the original image and the predicted action to the actual action. The dynamics loss is for training the sequence model to predict future representations, and the representation loss is used to make “representations become more predictable.”

#### Why it worked
The authors attribute the success of Dreamer due to a few factors, the first of which being its ability to imagine future scenarios. By utilizing a world model network to predict what outcomes will occur based on previous actions, Dreamer is capable of imagining what its actions will do and use those predictions to feed back to the actor and critic without the need for expert data or human intervention. While creating these world models for reinforcement learning has previously been a tough problem, the authors in Dreamer were able to employ optimizations and new strategies to various parts of the problem such as the architecture of the world model and different predictive loss (symlog) to overcome these challenges and train a robust algorithm.

#### Open Challenges
The authors of this paper claim that this algorithm paves the way for future research in teaching knowledgeable agents. The paper seems to revolve around applying the method in simulations and video games, but further testing should be done to accurately make this claim for real robots.

## Variant
### GenAug: https://arxiv.org/abs/2302.06671
Application: Data Augmentation

#### Idea
<img width="648" alt="Screenshot 2024-10-07 at 4 13 46 PM" src="https://github.com/user-attachments/assets/7d11a37e-06a7-458d-b726-e5574944c458">

Robot learning methods such as behavioral cloning or other imitation learning algorithms often require extensive amounts of domain specific expert data. In certain applications, it may be tedious or difficult to obtain such data, and the data may not scale well outside of the specific demonstrations that are given. To address this lack of training data for the robot, the authors leverage generative models to create more training data for robots to use in imitation learning tasks. Robotic tabletop manipulation tasks with a prompt are considered for this paper. Rather than relating to environmental prediction, GenAug appeals to environmental augmentation.

#### Key Contribution
The authors utilize existing image generation models to augment training data by altering certain aspects of the images. Aspects to be considered include the color and type of object to be grasped, the type of target to lay the object on, the addition or removal of extraneous “distractor” objects, and the background. Notably, these are the only possible augmentations considered in this paper, and they do not consider any other augmentations such as moving objects around in the scene. This is because any augmentation needs to maintain the consistency of the actions taken when compared to the original data, since they do not leverage any models that predict actions or states like the previous papers do. Here we discuss their methods for generating new grasping objects and targets: in-category generation and cross-category generation.

<img width="628" alt="Screenshot 2024-10-07 at 4 14 26 PM" src="https://github.com/user-attachments/assets/127476f5-affb-411c-aabd-faa64bdfe7e0">

In-category generation is simply altering the objects and targets to have different colors or patterns (blue, red, colorful, etc.), which is done through a masking process and with a depth-guided text-to-image inpainting model. Cross-category generation involves changing the objects to something else entirely (basket to bucket). Changing the object is more challenging since the newly rendered object needs to abide by the geometry of the environment while also being scaled correctly. The authors overcame this challenge by utilizing a dataset of existing meshes. They used randomly generated scales and poses to determine the correct perspective, and used a similar approach to the in-category method with a generative model to determine the visual features. By combining these, they were able to generate a new image with sufficient 3D consistency.

#### Open Challenges
This paper addresses how to increase training data for a specific pick and place application in a somewhat narrow way. Future work could address applying these additional training augmentations to various tasks, or even a method to make augmentations that is agnostic to the problem at hand. In addition, a method that could provide more training data in a way that also changes the actual required movements to the robot could probably make for an even stronger dataset.

#### Why it worked
In problems where a lack of training data is heavily detrimental to robot performance, GenAug is able to supplement existing data with a plethora of semantically meaningful augmentations. Compared to other imitation learning methods, the authors showed empirically that the extended dataset that GenAug gave access to allowed the robot to perform at a decently higher success rate on average. Notably, the authors mentioned that simply adding the same images back to the training set could only cause marginal increases in success, however adding the GenAug data was what caused a great increase in performance, indicating that “generating physically plausible scenes that are semantically meaningful is important.”

## Relation to Decision-Making for Robotics
We have covered several papers that discuss modifications and additions to reinforcement learning that not only learn a policy for their chosen problem, but can also determine how taking actions from a chosen policy could affect their future state and world. Indeed, some of these papers have explicit robotic applications directly from their experimentation. 

The future image similarity paper trained their robot for use in navigation tasks, by studying expert navigation data and using this data to predict how certain actions might affect how the 	image stream will be affected: Thus providing itself more training data to explore using RL. In the PlanGAN paper, the authors detail their experimentation results with navigation and various pick/place applications. Their data indicates that PlanGAN is capable of training a very good policy with far less environmental interaction than other methods, which is due to its ability to plan future trajectories. The GenAug paper is dedicated to pick and place applications, with the goal being to generate more image data so that pick and place robots make better decisions.

While the Dreamerv3 paper does not contain much robotics experimentation like the other papers, its findings are still relevant to decision making for robotics. The paper contains some simulated tests as well as some other tests in video game environments. The setup for their method is not particularly different from that in a real life setup: We still provide an input image stream and train a policy to trigger some actuators, and their world model should still generate feasible future images of the environment. It would be worthwhile to attempt to train Dreamerv3 on pick and place tasks like the other methods.

Overall, all of these papers address the problem of understanding a future environment in unique ways, and their approaches go in different directions. We believe that future experimentation should be done with classical robotic decision-making problems to compare these various methods in a streamlined manner.

## Table of Critiques
### Plan GAN
- This paper is very successful at minimizing the number of times that the agent needs to explore, thus reducing time and resources. The ability to see future trajectories all the way to the goal using GANs is a novel addition.
- I don’t think Plan GAN will ever be able to achieve Diamond in Minecraft, the reason being that once you are able to achieve a goal state then only your training dataset will be enhanced with the help of GANs, else it will work poorly then any model-free RL.
- What if a new goal is added into the environment, it will still try to overfit towards the existing goals.
- This method is based in prior work in Hindsight Experience Replay (HER), however this method will work in the case of new goal as it will use all existing transactions and rename existing goals in those transactions to g’ (new goal added)
### Future Image Similarity
- This method is the earliest paper in this list, so the use of CNNs for prediction in this way was probably pretty novel. The generated images based on the action are not very far off from the ground truth, and the success rate of this algorithm is increased compared to standard behavioral cloning.
- It is a smaller version of Plan GAN as it doesn't predict the whole trajectory.
- It will be overfitted towards the expert data and new goals added in the environment will not affect the trajectories.
- If the agent performs actions that are far out of the scope of the expert trajectories, the generated imagery will likely not be that accurate or helpful.
### Dreamer V3
- Dreamerv3 is definitely one of the more robust algorithms that we discussed, it works in a large variety of environments without much in the way of tuning, restructuring, or hyperparameters changes.
- People have tried multi-actor and multi-critic algorithms, but training more critics will just be overfitting most of the time and training more actors doesn’t make sense at all except in cases where your actions are totally contrast like acceleration vs braking in a model that is learning to drift. 
- So the idea of learning the next state instead, from the neural network, helped the algorithm to converge better.
- The consistency of this method is not immediately clear. The fact that it has to train three models simultaneously seems that the results might be very different depending on each environment, scenario, or even between runs. Further testing for the consistency might be appropriate.
### GenAug
- It takes care of one drawback of above “Future Image Similarity” & “Plan GAN” that these are not scalable to unseen environments.
- It also brings in the knowledge of pre-trained large language models while other papers only restricted themselves to robotic environments or expert trajectories.
- Doesn’t allow the robot to learn new actions or trajectories, since these are not considered in their augmentations.
